{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Курсовой проект для курса \"Python для Data Science\"\n",
    "\n",
    "Материалы к проекту (файлы): train.csv test.csv\n",
    "\n",
    "Задание: \n",
    "Используя данные из обучающего датасета (train.csv), построить модель для предсказания цен на недвижимость (квартиры). \n",
    "С помощью полученной модели, предсказать цены для квартир из тестового датасета (test.csv).\n",
    "\n",
    "Целевая переменная: Price\n",
    "Метрика качества: R2 - коэффициент детерминации (sklearn.metrics.r2_score)\n",
    "\n",
    "Требования к решению:\n",
    "R2 > 0.6\n",
    "Тетрадка Jupyter Notebook с кодом Вашего решения, названная по образцу {ФИО}_solution.ipynb, пример SShirkin_solution.ipynb\n",
    "Файл CSV с прогнозами целевой переменной для тестового датасета, названный по образцу {ФИО}_predictions.csv, пример SShirkin_predictions.csv \n",
    "Файл должен содержать два поля: Id, Price и в файле должна быть 5001 строка (шапка + 5000 предсказаний).\n",
    "\n",
    "Сроки сдачи: Cдать проект нужно в течение 72 часов после окончания последнего вебинара. Оценки работ, сданных до дедлайна, будут представлены в виде рейтинга, ранжированного по заданной метрике качества. Проекты, сданные после дедлайна или сданные повторно, не попадают в рейтинг, но можно будет узнать результат.\n",
    "\n",
    "Рекомендации для файла с кодом (ipynb):\n",
    "Файл должен содержать заголовки и комментарии (markdown)\n",
    "Повторяющиеся операции лучше оформлять в виде функций\n",
    "Не делать вывод большого количества строк таблиц (5-10 достаточно)\n",
    "По возможности добавлять графики, описывающие данные (около 3-5)\n",
    "Добавлять только лучшую модель, то есть не включать в код все варианты решения проекта\n",
    "Скрипт проекта должен отрабатывать от начала и до конца (от загрузки данных до выгрузки предсказаний)\n",
    "Весь проект должен быть в одном скрипте (файл ipynb).\n",
    "Допускается применение библиотек Python и моделей машинного обучения, которые были в данном курсе.\n",
    "\n",
    "Описание датасета: \n",
    "Id - идентификационный номер квартиры \n",
    "\n",
    "DistrictId - идентификационный номер района \n",
    "\n",
    "Rooms - количество комнат \n",
    "\n",
    "Square - площадь \n",
    "\n",
    "LifeSquare - жилая площадь \n",
    "\n",
    "KitchenSquare - площадь кухни \n",
    "\n",
    "Floor - этаж \n",
    "\n",
    "HouseFloor - количество этажей в доме \n",
    "\n",
    "HouseYear - год постройки дома \n",
    "\n",
    "Ecology_1, Ecology_2, Ecology_3 - экологические показатели местности \n",
    "\n",
    "Social_1, Social_2, Social_3 - социальные показатели местности \n",
    "\n",
    "Healthcare_1, Helthcare_2 - показатели местности, связанные с охраной здоровья \n",
    "\n",
    "Shops_1, Shops_2 - показатели, связанные с наличием магазинов, торговых центров \n",
    "\n",
    "Price - цена квартиры\n",
    "\n",
    "# РЕШЕНИЕ\n",
    "\n",
    "Загрузим нужные библиотеки\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Загрузим файлы к проекту: train.csv test.csv\n",
    "\n",
    "train = pd.read_csv('/Users/ekaterina/Downloads/project_task/train.csv')\n",
    "test = pd.read_csv('/Users/ekaterina/Downloads/project_task/test.csv')\n",
    "print('Материалы к проекту загружены!\\n')\n",
    "print(f'Размер массива Train:\\n{train.shape[0]}\\tквартир and {train.shape[1]} признаков\\n')\n",
    "print(f'Размер массива Test:\\n{test.shape[0]}\\tквартир and {test.shape[1]} признаков')\n",
    "\n",
    "Посмотрим первые 5 строк в датасетах:\n",
    "\n",
    "train.head()\n",
    "\n",
    "test.head()\n",
    "\n",
    "Посмотрим количество заполненных значений и типы данных:\n",
    "\n",
    "train.info(memory_usage='deep')\n",
    "\n",
    "test.info(memory_usage='deep')\n",
    "\n",
    "Оптимизируем использование памяти:\n",
    "\n",
    "train['Rooms'] = train['Rooms'].astype('int16')\n",
    "train['Floor'] = train['Floor'].astype('int16')\n",
    "train['DistrictId'] = train['DistrictId'].astype('int32')\n",
    "train['HouseFloor'] = train['HouseFloor'].astype('int32')\n",
    "train['HouseYear'] = train['HouseYear'].astype('int32')\n",
    "train['Square'] = train['Square'].astype('float32')\n",
    "train['LifeSquare'] = train['LifeSquare'].astype('float32')\n",
    "train['KitchenSquare'] = train['KitchenSquare'].astype('float32')\n",
    "train['Price'] = train['Price'].astype('float32')\n",
    "train['Healthcare_1'] = train['Healthcare_1'].astype('float32')\n",
    "train['Helthcare_2'] = train['Helthcare_2'].astype('int32')\n",
    "train['Social_1'] = train['Social_1'].astype('int32')\n",
    "train['Social_2'] = train['Social_2'].astype('int32')\n",
    "train['Social_3'] = train['Social_3'].astype('int32')\n",
    "train['Shops_1'] = train['Shops_1'].astype('int32')\n",
    "train['Ecology_1'] = train['Ecology_1'].astype('float32')\n",
    "\n",
    "train.info(memory_usage='deep')\n",
    "\n",
    "train.describe()\n",
    "\n",
    "# Поработаем с данными:\n",
    "\n",
    "0. Исключим признак id\n",
    "\n",
    "train = train[train.columns[1:]]\n",
    "\n",
    "1. HouseYear\n",
    "\n",
    "train[train['HouseYear']>2020]\n",
    "\n",
    "train.loc[train['HouseYear'] == 20052011, 'HouseYear'] = int((2005 + 2011) / 2)\n",
    "train.loc[train['HouseYear'] == 4968, 'HouseYear'] = 1968\n",
    "\n",
    "train['HouseYear'].unique()\n",
    "\n",
    "2. Square\n",
    "\n",
    "train.loc[(train['Square'] < 3)].transpose()\n",
    "\n",
    "Удалим строки с квартирами площадью меньше 3 м2\n",
    "\n",
    "train = train.drop(1608, axis=0)\n",
    "train = train.drop(6392, axis=0)\n",
    "train = train.drop(4853, axis=0)\n",
    "train = train.drop(8283, axis=0)\n",
    "train = train.drop(9294, axis=0)\n",
    "\n",
    "train.loc[(train['Square'] < 3)]\n",
    "\n",
    "Квартиры с площадью больше, чем 600м2 скорее всего опечатки, разделим на 10\n",
    "\n",
    "train.loc[(train['Square'] > 600)]\n",
    "\n",
    "train.loc[train['Square'] > 600, 'Square'] = train.loc[train['Square'] > 600, 'Square']/10\n",
    "train.loc[train['LifeSquare'] > 600, 'LifeSquare'] = train.loc[train['LifeSquare'] > 600, 'LifeSquare']/10\n",
    "\n",
    "train.loc[4262]\n",
    "\n",
    "train.loc[6977]\n",
    "\n",
    "Квартиры с площадью мельше, чем 6м2 скорее всего опечатки, умножим на 10\n",
    "\n",
    "train.loc[(train['Square'] < 6)].transpose()\n",
    "\n",
    "train.loc[train['Square']  < 6, 'Square'] = train.loc[train['Square'] < 6, 'Square']*10\n",
    "\n",
    "3. LifeSquare\n",
    "\n",
    "Заполним недостающие данные по жилой площади и показатели местности, связанные с охраной здоровья\n",
    "\n",
    "train[train['LifeSquare'].isnull()]\n",
    "\n",
    "train.loc[(train['LifeSquare'].isnull(), 'LifeSquare')] = train['Square']-10\n",
    "\n",
    "train.loc[(train['LifeSquare'] - train['Square'] >= 3)]\n",
    "\n",
    "train.loc[(train['Square'] < train['LifeSquare'], 'LifeSquare')] = train['Square']-10\n",
    "\n",
    "train.loc[(train['Square'] < train['KitchenSquare'])]\n",
    "\n",
    "train['KitchenSquare'].median()\n",
    "\n",
    "train.loc[(train['Square'] < train['KitchenSquare']), 'KitchenSquare'] = train['KitchenSquare'].median()\n",
    "train.loc[(train['KitchenSquare'] == 0), 'KitchenSquare'] = train['KitchenSquare'].median()\n",
    "\n",
    "train.loc[(train['Healthcare_1'].isnull(), 'Healthcare_1')] = train['Healthcare_1'].median()\n",
    "\n",
    "train[train['Healthcare_1'].isnull()]\n",
    "\n",
    "4. Rooms & Floors\n",
    "\n",
    "train.loc[((train['Rooms'] > 5)&(train['Square'] < 70))|(train['Rooms'] == 0)]\n",
    "\n",
    "train.loc[(((train['Rooms'] > 5)&(train['Square'] < 70))|(train['Rooms'] == 0)), 'Rooms'] = train['Square']//20\n",
    "\n",
    "train.loc[1454]\n",
    "\n",
    "train.loc[(train['HouseFloor'] == 0)]\n",
    "\n",
    "train['HouseFloor'].median()\n",
    "\n",
    "Так как медиана меньше некоторых этажей, она не подходит для замены нулей\n",
    "\n",
    "train.loc[(train['HouseFloor'] == 0), 'HouseFloor'] = train['Floor']\n",
    "\n",
    "train.loc[((train['HouseFloor'] > 50))]\n",
    "\n",
    "train.loc[((train['HouseFloor'] > 50)), 'HouseFloor'] = train['HouseFloor']//10\n",
    "\n",
    "5. Заполним оставшиеся пропуски модой или медианой\n",
    "\n",
    "#train['DistrictId'].median()\n",
    "\n",
    "train['Ecology_1'].mode()\n",
    "\n",
    "train['Social_1'].median()\n",
    "\n",
    "train['Social_3'].median()\n",
    "\n",
    "train['Healthcare_1'].median()\n",
    "\n",
    "#train['Healthcare_2'].mode()\n",
    "\n",
    "train['Shops_1'].median()\n",
    "\n",
    "train.loc[train['Shops_1'] == 0].transpose()\n",
    "\n",
    "train.loc[(train['DistrictId'] == 0), 'DistrictId'] = train['DistrictId'].median()\n",
    "\n",
    "#train.loc[(train['Ecology_1'] == 0), 'Ecology_1'] = train['Ecology_1'].mode()\n",
    "\n",
    "train.loc[(train['Social_1'] == 0), 'Social_1'] = train['Social_1'].median()\n",
    "\n",
    "train.loc[(train['Social_3'] == 0), 'Social_3'] = train['Social_3'].median()\n",
    "\n",
    "train.loc[(train['Healthcare_1'] == 0), 'Healthcare_1'] = train['Healthcare_1'].median()\n",
    "\n",
    "#train.loc[(train['Helthcare_2'] == 0), 'Helthcare_2'] = train['Helthcare_2'].median()\n",
    "\n",
    "\n",
    "train.loc[(train['Shops_1'] == 0), 'Shops_1'] = train['Shops_1'].median()\n",
    "\n",
    "train['Shops_1'].unique()\n",
    "\n",
    "6. Преобразуем категориальные признаки в бинарные:\n",
    "\n",
    "    -получим доп.таблицы\n",
    "    \n",
    "    -присоединим их к датасету\n",
    "    \n",
    "    -удалим столбцы с нечисловыми значениями\n",
    "\n",
    "train['Ecology_2'].unique()\n",
    "\n",
    "train['Shops_2'].unique()\n",
    "\n",
    "pd.get_dummies(train['Ecology_2'])[:2]\n",
    "pd.get_dummies(train['Ecology_3'])[:2]\n",
    "pd.get_dummies(train['Shops_2'])[:2]\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train['Ecology_2'])], axis = 1)\n",
    "train = pd.concat([train, pd.get_dummies(train['Ecology_3'])], axis = 1)\n",
    "train = pd.concat([train, pd.get_dummies(train['Shops_2'])], axis = 1)\n",
    "\n",
    "train = train.drop(['Ecology_2'], axis = 1)\n",
    "train = train.drop(['Ecology_3'], axis = 1)\n",
    "train = train.drop(['Shops_2'], axis = 1)\n",
    "\n",
    "Посмотрим на обработанные данные\n",
    "\n",
    "train.info(memory_usage='deep')\n",
    "\n",
    "train.describe().transpose()\n",
    "\n",
    "Сохраним обработанные данные в файл\n",
    "\n",
    "train.to_csv('/Users/ekaterina/Downloads/project_task/train_prepared.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# Визуализируем подготовленные данные\n",
    "\n",
    "Посмотрим корреляцию данных:\n",
    "\n",
    "corrmat = train.loc[:, (train.columns != 'A')&(train.columns != 'B')].corr()\n",
    "plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=0.5, square=True)\n",
    "\n",
    "Видим наибольшую корреляцию между ценой и площадью квартиры, количеством комнат, а также жилой площадью. \n",
    "\n",
    "Среднее влияние на цену оказывают параметры: \n",
    "    район,  площадь кухни, социальные показатели местности, показатели местности, связанные с охраной здоровья, с наличием магазинов, торговых центров\n",
    "\n",
    "Малое влияние на цену оказывают параметры:\n",
    "    этаж, количество этажей в доме, год строительства\n",
    "    \n",
    "Экологические показатели местности практически не влияют на цену, поэтому при прогнозе их можно не учитывать.\n",
    "\n",
    "Похожую картину мы видим на следующей диаграмме:\n",
    "\n",
    "corrmat = train.loc[:, train.columns != 'Price'].corrwith(\n",
    "    train['Price']).abs().sort_values(ascending=True)[0:]\n",
    "plt.bar(corrmat.values, corrmat.index)\n",
    "plt.title('Correlation to Price')\n",
    "plt.xticks()\n",
    "plt.show()\n",
    "\n",
    "df = pd.read_csv('/Users/ekaterina/Downloads/project_task/train_prepared.csv')\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "df.info(memory_usage='deep')\n",
    "\n",
    "Отбор примеров\n",
    "\n",
    "df['Price'].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "df['Price'].hist(bins=30)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('Price')\n",
    "\n",
    "plt.title('Distribution of Price')\n",
    "plt.show()\n",
    "\n",
    "Отбор признаков\n",
    "\n",
    "df.columns.tolist()\n",
    "\n",
    "feature_names = ['DistrictId', 'Rooms','Square','LifeSquare',\n",
    " 'KitchenSquare',\n",
    " 'Floor',\n",
    " 'HouseFloor',\n",
    " 'HouseYear',\n",
    " 'Ecology_1',\n",
    " 'Social_1',\n",
    " 'Social_2',\n",
    " 'Social_3',\n",
    " 'Healthcare_1',\n",
    " 'Helthcare_2',\n",
    " 'Shops_1',\n",
    " 'A','B','A.1','B.1','A.2','B.2']\n",
    "\n",
    "target_name = 'Price'\n",
    "\n",
    "df = df[feature_names + [target_name]]\n",
    "df.head()\n",
    "\n",
    "Стандартизация признаков\n",
    "\n",
    "df.mean()\n",
    "\n",
    "feature_names_for_stand = df[feature_names].select_dtypes(include='float64').columns.tolist()\n",
    "feature_names_for_stand\n",
    "\n",
    "# MinMaxScaler\n",
    "# fit - преобразование данных / обучение модели\n",
    "# predict / transform\n",
    "\n",
    "scaler = StandardScaler()\n",
    "stand_features = scaler.fit_transform(df[feature_names_for_stand])\n",
    "\n",
    "df[feature_names_for_stand] = pd.DataFrame(stand_features, columns=feature_names_for_stand)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.mean()\n",
    "\n",
    "df.std()\n",
    "\n",
    "Обучающий датасет готов\n",
    "\n",
    "Разобьем данные на тренировочный и тестовый датасеты:\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df[target_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=100)\n",
    "\n",
    "Построение базовых моделей и выбор лучшей\n",
    "\n",
    "Загрузим метрики\n",
    "\n",
    "MAE - Mean Absolute Error \n",
    "\n",
    "MSE - Mean Squared Error\n",
    "\n",
    "RMSE - Root Mean Squared Error\n",
    "\n",
    "R2 - coefficient of determination\n",
    "\n",
    "from sklearn.metrics import r2_score as r2, mean_absolute_error as mae, mean_squared_error as mse\n",
    "from math import sqrt\n",
    "\n",
    "def evaluate_preds(true_values, pred_values):\n",
    "    print(\"R2:\\t\" + str(round(r2(true_values, pred_values), 3)) + \"\\n\" +\n",
    "          \"MAE:\\t\" + str(round(mae(true_values, pred_values), 3)) + \"\\n\" +\n",
    "          \"MSE:\\t\" + str(round(mse(true_values, pred_values), 3)) + \"\\n\" +\n",
    "          \"RMSE:\\t\" + str(round(sqrt(mse(true_values, pred_values)), 3)))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    sns.scatterplot(x=pred_values, y=true_values)\n",
    "    \n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('True vs Predicted values')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "Построим модель линейной регрессии:\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = lr_model.predict(X_train)\n",
    "evaluate_preds(y_train, y_train_preds)\n",
    "\n",
    "y_test_preds = lr_model.predict(X_test)\n",
    "evaluate_preds(y_test, y_test_preds)\n",
    "\n",
    "Random Forest Regressor\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = rf_model.predict(X_train)\n",
    "evaluate_preds(y_train, y_train_preds)\n",
    "\n",
    "y_test_preds = rf_model.predict(X_test)\n",
    "evaluate_preds(y_test, y_test_preds)\n",
    "\n",
    "Отложенная выборка\n",
    "\n",
    "def evaluate_preds_sec(train_true_values, train_pred_values, test_true_values, test_pred_values):\n",
    "    \"\"\"\n",
    "    # дописать документацию0\n",
    "    \"\"\"\n",
    "    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))\n",
    "    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))\n",
    "    \n",
    "    plt.figure(figsize=(18,10))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    sns.scatterplot(x=train_pred_values, y=train_true_values)\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('Train sample prediction')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    sns.scatterplot(x=test_pred_values, y=test_true_values)\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('True values')\n",
    "    plt.title('Test sample prediction')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "y_train_preds = rf_model.predict(X_train.fillna(-9999))\n",
    "y_test_preds = rf_model.predict(X_test)\n",
    "\n",
    "evaluate_preds_sec(y_train, y_train_preds, y_train, y_train_preds)\n",
    "\n",
    "**Перекрёстная проверка**\n",
    "\n",
    "cv_score = cross_val_score(rf_model, X.fillna(-9999), y, scoring='r2', cv=KFold(n_splits=5, shuffle=True, random_state=21))\n",
    "cv_score\n",
    "\n",
    "cv_score.mean(), cv_score.std()\n",
    "\n",
    "\n",
    "cv_score.mean() - cv_score.std(), cv_score.mean() + cv_score.std()\n",
    "\n",
    "**Важность признаков**\n",
    "\n",
    "feature_importances = pd.DataFrame(zip(X_train.columns, rf_model.feature_importances_), \n",
    "                                   columns=['feature_name', 'importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "Gradient Boosting Regressor\n",
    "\n",
    "gb_model = GradientBoostingRegressor()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = gb_model.predict(X_train)\n",
    "evaluate_preds(y_train, y_train_preds)\n",
    "\n",
    "y_test_preds = gb_model.predict(X_test)\n",
    "evaluate_preds(y_test, y_test_preds)\n",
    "\n",
    "Настройка и оценка финальной модели\n",
    "\n",
    "Подбор гиперпараметров\n",
    "\n",
    "0.6*X_train.shape[1]\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model\n",
    "\n",
    "params = {'n_estimators':[50, 100, 200, 400], \n",
    "          'max_depth':[3, 5, 7, 10]}\n",
    "\n",
    "gs = GridSearchCV(gb_model, params, scoring='r2', cv=KFold(n_splits=3, random_state=42, shuffle=True), n_jobs=-1)\n",
    "gs.fit(X, y)\n",
    "\n",
    "gs.best_params_\n",
    "\n",
    "gs.best_score_\n",
    "\n",
    "`GridSearchCV` - это классификатор, который строится на основе модели `estimator`, пробегая все комбинации значений из `param_grid`. Для каждой комбинации параметров по кросс-валидации на указанном количестве _фолдов_ считается метрика, указанная в `scoring`. Наконец, выбирается та комбинация параметров, при которой выбранная метрика оказалась максимальной, и дальше для предсказания используется именно этот набор параметров.\n",
    "\n",
    "Обучение и оценка модели\n",
    "\n",
    "final_model = GradientBoostingRegressor(n_estimators=400, max_depth=7, random_state=21)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_preds = final_model.predict(X_train)\n",
    "evaluate_preds(y_train, y_train_preds)\n",
    "\n",
    "y_test_preds = final_model.predict(X_test)\n",
    "evaluate_preds(y_test, y_test_preds)\n",
    "\n",
    "Важность признаков\n",
    "\n",
    "feature_importances = pd.DataFrame(zip(X_train.columns, final_model.feature_importances_), \n",
    "                                   columns=['feature_name', 'importance'])\n",
    "\n",
    "feature_importances.sort_values(by='importance', ascending=False)\n",
    "\n",
    "Сохранение модели\n",
    "\n",
    "**Scaler**\n",
    "\n",
    "df.to_csv('/Users/ekaterina/Downloads/project_task/scaler.pkl')\n",
    "\n",
    "with open('/Users/ekaterina/Downloads/project_task/scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "with open('/Users/ekaterina/Downloads/project_task/scaler.pkl', \"rb\") as file:\n",
    "    new_scaler = pickle.load(file)\n",
    "\n",
    "**Model**\n",
    "\n",
    "df.to_csv('/Users/ekaterina/Downloads/project_task/model.pkl')\n",
    "\n",
    "with open('/Users/ekaterina/Downloads/project_task/model.pkl', 'wb') as file:\n",
    "    pickle.dump(final_model, file)\n",
    "\n",
    "Результат:\n",
    "\n",
    "\n",
    "\n",
    "Выводы:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Выгрузка данных:\n",
    "\n",
    "test[['Id', 'Price']].to_csv('/Users/ekaterina/Downloads/project_task/Macerszmidt_predictions.csv', index=None)\n",
    "\n",
    "\n",
    "\n",
    "## Алгоритм t-SNE\n",
    "\n",
    "Алгоритм _t-SNE_ (_t-distributed Stochastic Neighbor Embedding_ или _Стохастическое вложение соседей с t-распределением_) позволяет понижать размерность данных до двух или трёх измерений, что позволяет визуализировать данные на двумерных и трёхмерных графиках. Изучая графики, можно, например, понять, на сколько кластеров адекватно разбивать данные, а также оценить уже выполненное разбиение на кластеры.\n",
    "\n",
    "Итак, перейдём к использованию t-SNE. \n",
    "Зададим параметр `n_components=2`, чтобы получить данные с двумя признаками. \n",
    "Параметр `learning_rate` влияет на то, как плотно будут располагаться точки. \n",
    "Рекомендуется задавать его в диапазоне от 10 до 1000.\n",
    "\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "stand_features = scaler.fit_transform(df[feature_names_for_stand])\n",
    "\n",
    "df[feature_names_for_stand] = pd.DataFrame(stand_features, columns=feature_names_for_stand)\n",
    "\n",
    "tsne = TSNE(n_components=2, learning_rate=150, random_state=100)\n",
    "\n",
    "X_train_tsne = tsne.fit_transform(df[feature_names_for_stand])\n",
    "\n",
    "print('До:\\t{}'.format(df[feature_names_for_stand].shape))\n",
    "print('После:\\t{}'.format(X_train_tsne.shape))\n",
    "\n",
    "Мы видим, что число признаков уменьшилось с 10 до 2. Теперь можно визуализировать наши данные на плоскости.\n",
    "\n",
    "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "По графику видно, что данные можно разбить как минимум на 2 кластера. Попробуем сделать это с помощью уже известного нам метода K-means и ещё раз построим график, но уже с полученными метками кластеров.\n",
    "\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "labels_train = kmeans.fit_predict(df[feature_names_for_stand])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=labels_train)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Как мы видим, при кластеризации мы получили практически такое же разбиение, как то, которое можно было наблюдать в результате t-SNE.\n",
    "\n",
    "Для тестовой выборки получить аналогичный график нам не удастся, поскольку у алгоритма t-SNE нет метода `.transform`. Однако, с помощью алгоритма K-means мы можем получить метки кластеров для тестовой выборки с помощью метода `.predict` и использовать их:\n",
    "\n",
    "Давайте посмотрим на объекты из разных кластеров, чтобы попытаться понять, почему алгоритм t-SNE выделил две группы. Например, посмотрим на среднюю цену недвижимости во всей выборке и в отдельных кластерах.\n",
    "\n",
    "\n",
    "\n",
    "labels_test = kmeans.predict(stand_features)\n",
    "\n",
    "y_train.mean()\n",
    "\n",
    "y_train[labels_train == 1].mean()\n",
    "\n",
    "plt.hist(y_train[labels_train == 0], bins=20, density=True, alpha=0.5)\n",
    "plt.hist(y_train[labels_train == 1], bins=20, density=True, alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "labels_test = kmeans.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "y_train.mean()\n",
    "\n",
    "y_train[labels_train == 0].mean()\n",
    "\n",
    "y_train[labels_train == 1].mean()\n",
    "\n",
    "\n",
    "\n",
    "вот здесь выдает ошибку и не дает дальше идти"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
